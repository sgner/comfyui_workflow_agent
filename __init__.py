import threading
import time
import logging
import os
import sys
import json
import hashlib

# 1. ç¯å¢ƒé…ç½®
# å¼ºåˆ¶ä½¿ç”¨å›½å†…é•œåƒ
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# ç¦ç”¨ Chroma é¥æµ‹
os.environ["ANONYMIZED_TELEMETRY"] = "False"
os.environ["CHROMA_SERVER_NOINTERACTIVE"] = "True"

logging.basicConfig(format="%(asctime)s [WorkflowAgent] %(message)s", level=logging.INFO)
logger = logging.getLogger("WorkflowAgent")

# 2. å¯¼å…¥æ¨¡å—
try:
    from custom_nodes.comfyui_workflow_agent.backend.chatbot.catalog.scanner import NodeScanner
    from custom_nodes.comfyui_workflow_agent.backend.chatbot.catalog.workflow_parser import WorkflowParser
    from custom_nodes.comfyui_workflow_agent.backend.chatbot.catalog.node_formatter import KnowledgeBaseFormatter
    from custom_nodes.comfyui_workflow_agent.backend.chatbot.catalog.vdb import ChromaVectorStore
except ImportError as e:
    logger.error(f"âŒ Failed to import: {e}")
    NodeScanner = None
    WorkflowParser = None
    KnowledgeBaseFormatter = None
    ChromaVectorStore = None

# 3. çŠ¶æ€ç®¡ç†
GLOBAL_VECTOR_STORE = None


class SystemStatus:
    INITIALIZING = "initializing"
    READY = "ready"
    ERROR = "error"


CURRENT_STATUS = SystemStatus.INITIALIZING
STATUS_MESSAGE = "System is initializing..."


def get_system_status():
    if CURRENT_STATUS != SystemStatus.READY:
        raise RuntimeError(f"Service Unavailable: {STATUS_MESSAGE}")
    return True


# =========================================================
# å“ˆå¸Œè¾…åŠ©å‡½æ•°
# =========================================================

def get_workflow_fingerprint(wf_data):
    """è®¡ç®—å·¥ä½œæµæŒ‡çº¹: æ–‡ä»¶å + åŸå§‹å†…å®¹"""
    content = wf_data['filename'] + wf_data.get('raw_json', '')
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def get_node_fingerprint(node_data):
    """è®¡ç®—èŠ‚ç‚¹æŒ‡çº¹: ID + IOå®šä¹‰ + æè¿° + Readme"""
    content_str = (
        str(node_data['id']) +
        str(node_data['input_types']) +
        str(node_data['return_types']) +
        str(node_data['description']) +
        str(node_data['readme_snippet'])
    )
    return hashlib.md5(content_str.encode('utf-8')).hexdigest()


def load_local_hashes(path):
    if os.path.exists(path):
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return {}
    return {}


def save_local_hashes(path, hash_map):
    with open(path, 'w', encoding='utf-8') as f:
        json.dump(hash_map, f, indent=2)


def ensure_vector_store():
    """å•ä¾‹æ¨¡å¼è·å–æ•°æ®åº“å®ä¾‹"""
    global GLOBAL_VECTOR_STORE
    if GLOBAL_VECTOR_STORE is None:
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            db_path = os.path.join(current_dir, "vector_db")
            GLOBAL_VECTOR_STORE = ChromaVectorStore(db_path)
        except Exception as e:
            logger.error(f"âŒ Failed to initialize ChromaDB: {e}")
            return None
    return GLOBAL_VECTOR_STORE


# =========================================================
# åå°ä»»åŠ¡ä¸»é€»è¾‘
# =========================================================

def background_indexing_task():
    global CURRENT_STATUS, STATUS_MESSAGE

    delay_seconds = 5
    logger.info(f"â³ Scanner scheduled in {delay_seconds}s...")
    time.sleep(delay_seconds)

    # æ£€æŸ¥æ ¸å¿ƒä¾èµ–
    if NodeScanner is None or ChromaVectorStore is None:
        CURRENT_STATUS = SystemStatus.ERROR
        STATUS_MESSAGE = "Missing dependencies"
        return

    # åˆå§‹åŒ–æ•°æ®åº“è¿æ¥ (åªåšä¸€æ¬¡)
    if not ensure_vector_store():
        CURRENT_STATUS = SystemStatus.ERROR
        return

    # ------------------------------------------------------------------
    # ä»»åŠ¡ 1: å¢é‡æ‰«æå·¥ä½œæµ (Workflow Templates)
    # ------------------------------------------------------------------
    try:
        logger.info("ğŸ” [Catalog] Scanning example workflows...")

        base_path = os.path.dirname(os.path.abspath(__file__))
        examples_dir = os.path.join(base_path, "example_workflows")
        hash_file = os.path.join(base_path, "workflow_fingerprints.json")

        if not os.path.exists(examples_dir):
            os.makedirs(examples_dir)

        # æ‰«æ
        wf_scanner = WorkflowParser(examples_dir)
        current_workflows = wf_scanner.scan()

        # åŠ è½½æ—§å“ˆå¸Œ
        old_hashes = load_local_hashes(hash_file)
        new_hashes = {}
        to_upsert = []
        to_delete = []

        # Diff è®¡ç®—
        for wf in current_workflows:
            fid = wf['filename']
            fingerprint = get_workflow_fingerprint(wf)
            new_hashes[fid] = fingerprint

            if (fid not in old_hashes) or (old_hashes[fid] != fingerprint):
                to_upsert.append(wf)

        for old_id in old_hashes:
            if old_id not in new_hashes:
                to_delete.append(old_id)

        # æ‰§è¡Œæ›´æ–°
        if not old_hashes and current_workflows:
            logger.info(f"ğŸ†• [Workflow] First run. Indexing {len(current_workflows)} templates...")
            GLOBAL_VECTOR_STORE.process_workflow_update(current_workflows, [])
        elif to_upsert or to_delete:
            logger.info(f"ğŸ”„ [Workflow] Syncing: {len(to_upsert)} changed, {len(to_delete)} deleted.")
            GLOBAL_VECTOR_STORE.process_workflow_update(to_upsert, to_delete)
        else:
            logger.info("âš¡ [Workflow] No changes detected.")

        # ä¿å­˜çŠ¶æ€
        save_local_hashes(hash_file, new_hashes)

    except Exception as e:
        logger.error(f"âŒ [Catalog] Workflow scan failed: {e}")
        # æ³¨æ„ï¼šå·¥ä½œæµæ‰«æå¤±è´¥ä¸åº”é˜»æ­¢èŠ‚ç‚¹æ‰«æ

    # ------------------------------------------------------------------
    # ä»»åŠ¡ 2: å¢é‡æ‰«æèŠ‚ç‚¹ (Nodes)
    # ------------------------------------------------------------------
    try:
        logger.info("ğŸ” [Catalog] Scanning nodes...")
        scanner = NodeScanner()

        # æ™ºèƒ½é‡è¯•é˜²æ­¢å­—å…¸å˜åŒ–
        current_nodes_list = []
        for _ in range(3):
            try:
                current_nodes_list = scanner.scan()
                break
            except RuntimeError:
                time.sleep(1)

        if not current_nodes_list:
            logger.warning("âš ï¸ No nodes scanned.")
            # å³ä½¿æ²¡æ‰«åˆ°èŠ‚ç‚¹ï¼Œç³»ç»Ÿä¹Ÿç®— Ready (å¯èƒ½æ˜¯ç”¨æˆ·çœŸçš„æ²¡è£…æ’ä»¶)
            CURRENT_STATUS = SystemStatus.READY
            return

        base_path = os.path.dirname(os.path.abspath(__file__))
        hash_file_path = os.path.join(base_path, "node_fingerprints.json")

        # åŠ è½½æ—§å“ˆå¸Œ
        old_hashes = load_local_hashes(hash_file_path)
        new_hashes = {}
        nodes_to_upsert = []
        ids_to_delete = []

        # Diff è®¡ç®—
        for node in current_nodes_list:
            nid = str(node['id'])
            fingerprint = get_node_fingerprint(node)
            new_hashes[nid] = fingerprint

            if (nid not in old_hashes) or (old_hashes[nid] != fingerprint):
                nodes_to_upsert.append(node)

        for old_id in old_hashes:
            if old_id not in new_hashes:
                ids_to_delete.append(old_id)

        # æ£€æŸ¥æ˜¯å¦éœ€è¦å…¨é‡é‡å»º (é¦–æ¬¡è¿è¡Œæˆ–æ•°æ®åº“æ–‡ä»¶ä¸¢å¤±)
        # æ³¨æ„ï¼šè¿™é‡Œå¤ç”¨ ensure_vector_store() å¾—åˆ°çš„å®ä¾‹ï¼Œä¸å†é‡å¤åˆå§‹åŒ–

        is_fresh_start = not old_hashes

        if is_fresh_start:
            logger.info("ğŸ†• [Node] First run. Indexing all nodes...")
            docs = KnowledgeBaseFormatter.to_markdown(current_nodes_list)
            # ä½¿ç”¨å¢é‡æ¥å£ä¼ å…¥æ‰€æœ‰æ•°æ®ï¼Œæ•ˆæœç­‰åŒäºå…¨é‡
            GLOBAL_VECTOR_STORE.process_incremental_update(docs, [])
        else:
            if not nodes_to_upsert and not ids_to_delete:
                logger.info("âš¡ [Node] No changes detected.")
            else:
                logger.info(f"ğŸ”„ [Node] Syncing: {len(nodes_to_upsert)} changed, {len(ids_to_delete)} deleted.")
                docs_to_upsert = []
                if nodes_to_upsert:
                    docs_to_upsert = KnowledgeBaseFormatter.to_markdown(nodes_to_upsert)
                GLOBAL_VECTOR_STORE.process_incremental_update(docs_to_upsert, ids_to_delete)

        # ä¿å­˜çŠ¶æ€
        save_local_hashes(hash_file_path, new_hashes)

        CURRENT_STATUS = SystemStatus.READY
        STATUS_MESSAGE = "Ready"
        logger.info(f"âœ… [Catalog] All sync tasks complete.")

    except Exception as e:
        CURRENT_STATUS = SystemStatus.ERROR
        STATUS_MESSAGE = str(e)
        logger.error(f"âŒ [Catalog] Node scan failed: {e}")
        import traceback
        traceback.print_exc()


def start_scanner():
    if hasattr(sys, "_comfy_catalog_scanner_active"): return
    sys._comfy_catalog_scanner_active = True

    scan_thread = threading.Thread(target=background_indexing_task, name="ComfyUI_NodeScanner")
    scan_thread.daemon = True
    scan_thread.start()
    logger.info("ğŸš€ [Catalog] Scanner thread initiated.")


start_scanner()

WEB_DIRECTORY = "./web"
NODE_CLASS_MAPPINGS = {}
__all__ = ["NODE_CLASS_MAPPINGS", "WEB_DIRECTORY", "GLOBAL_VECTOR_STORE", "get_system_status"]
