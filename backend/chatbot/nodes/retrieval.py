import json
import os

from custom_nodes.comfyui_workflow_agent.backend.chatbot.state import BaseState
import logging
from typing import List, Dict, Any
# å¯¼å…¥å…¨å±€å•ä¾‹æ•°æ®åº“å’ŒçŠ¶æ€æ£€æŸ¥
from custom_nodes.comfyui_workflow_agent.backend.chatbot.catalog.vdb import ChromaVectorStore
# from custom_nodes.comfyui_workflow_agent import GLOBAL_VECTOR_STORE

logger = logging.getLogger("WorkflowAgent")

import json


def is_workflow_empty(workflow_data: dict | str | None) -> bool:
    """
    åˆ¤æ–­ ComfyUI å·¥ä½œæµæ˜¯å¦ä¸ºç©ºï¼ˆæ²¡æœ‰èŠ‚ç‚¹ï¼‰ã€‚
    å…¼å®¹ UI æ ¼å¼ï¼ˆåŒ…å« nodes åˆ—è¡¨ï¼‰å’Œ API æ ¼å¼ï¼ˆå­—å…¸ï¼‰ã€‚
    """
    if not workflow_data:
        return True

    # 1. å¤„ç†å­—ç¬¦ä¸²è¾“å…¥
    if isinstance(workflow_data, str):
        try:
            workflow_data = json.loads(workflow_data)
        except Exception:
            return True  # è§£æå¤±è´¥è§†ä¸ºæ— æ•ˆ/ç©º

    # 2. UI æ ¼å¼åˆ¤ç©º (ä½ çš„ä¾‹å­å±äºè¿™ç§)
    # ç‰¹å¾ï¼šæœ‰ä¸€ä¸ª "nodes" åˆ—è¡¨
    if "nodes" in workflow_data:
        nodes = workflow_data["nodes"]
        return len(nodes) == 0

    # 3. API æ ¼å¼åˆ¤ç©º
    # ç‰¹å¾ï¼šé¡¶å±‚å­—å…¸çš„ key æ˜¯èŠ‚ç‚¹ ID (æ•°å­—å­—ç¬¦ä¸²)ï¼Œvalue æ˜¯èŠ‚ç‚¹å†…å®¹
    # æˆ‘ä»¬éœ€è¦æ’é™¤æ‰ "id", "extra", "version" ç­‰éèŠ‚ç‚¹å­—æ®µ
    # é€šå¸¸ API æ ¼å¼ä¸åƒ UI æ ¼å¼é‚£æ ·ä¿ç•™ metadataï¼Œå®ƒå°±æ˜¯çº¯èŠ‚ç‚¹ ID æ˜ å°„
    # ä½†ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ˜¯å¦åŒ…å« "class_type" æˆ– "inputs" è¿™ç§ç‰¹å¾ key
    if isinstance(workflow_data, dict):
        # å¦‚æœæ˜¯ç©ºçš„ {}
        if not workflow_data:
            return True

        # éå† valueï¼Œçœ‹æ˜¯å¦åƒä¸€ä¸ªèŠ‚ç‚¹
        for key, val in workflow_data.items():
            if isinstance(val, dict) and ("class_type" in val or "inputs" in val):
                return False  # åªè¦å‘ç°ä¸€ä¸ªèŠ‚ç‚¹ï¼Œå°±ä¸ä¸ºç©º

        # åªæœ‰ metadata å­—æ®µï¼Œæ²¡æœ‰èŠ‚ç‚¹
        return True

    return True


def extract_node_types_from_workflow(workflow_data: dict | str) -> list[str]:
    """è¾…åŠ©å‡½æ•°ï¼šä»ç”¨æˆ·æä¾›çš„å·¥ä½œæµä¸­æå–æ‰€æœ‰èŠ‚ç‚¹ç±»å‹"""
    try:
        if isinstance(workflow_data, str):
            workflow_data = json.loads(workflow_data)

        node_types = set()
        # å…¼å®¹ API æ ¼å¼ (Dict[ID, Node]) å’Œ UI æ ¼å¼ (List[Node])
        nodes = workflow_data.get("nodes", []) if "nodes" in workflow_data else workflow_data.values()

        for node in nodes:
            ntype = node.get("type") or node.get("class_type")
            if ntype:
                node_types.add(ntype)
        return list(node_types)
    except Exception as e:
        logger.error(f"Error parsing current workflow: {e}")
        return []


def search_knowledge_node(state: BaseState) -> Dict[str, Any]:
    """
    LangGraph èŠ‚ç‚¹: çŸ¥è¯†æ£€ç´¢
    ç­–ç•¥ï¼š
    1. å¦‚æœæœ‰å½“å‰å·¥ä½œæµ -> æå–å…¶ä¸­èŠ‚ç‚¹ + è§„åˆ’å»ºè®®çš„èŠ‚ç‚¹ -> ç²¾ç¡®æ£€ç´¢èŠ‚ç‚¹è¯¦æƒ…
    2. å¦‚æœæ— å·¥ä½œæµ -> æ£€ç´¢å·¥ä½œæµæ¨¡ç‰ˆ -> æå–æ¨¡ç‰ˆèŠ‚ç‚¹ + è§„åˆ’å»ºè®®çš„èŠ‚ç‚¹ -> ç²¾ç¡®æ£€ç´¢èŠ‚ç‚¹è¯¦æƒ…
    3. å…œåº• -> æ¨¡ç³Šæ£€ç´¢
    """
    print("\n" + "=" * 50)
    print("ğŸ—„ï¸ [Search] Starting Knowledge Retrieval Phase")
    print("=" * 50)

    # 1. è·å–æ„å›¾æ•°æ®
    intent_json = state.intent_result
    user_intent = intent_json.get("user_intent", {})
    planning = intent_json.get("planning_suggestions", {})

    print(f"ğŸ“‹ User Intent: {user_intent.get('core_function', 'N/A')}")
    print(f"ğŸ“‹ Details: {user_intent.get('details', [])}")

    # å¦‚æœæ²¡æœ‰æ„å›¾æˆ–è¿˜åœ¨æ¾„æ¸…é˜¶æ®µï¼Œè·³è¿‡æ£€ç´¢
    if not user_intent or user_intent.get("clarification_needed", False):
        print("â„¹ï¸ [Search] Clarification needed. Skipping retrieval.")
        return {"retrieved_knowledge": []}

    ######################################################
    current_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    db_path = os.path.join(current_dir, "vector_db")
    store = ChromaVectorStore(db_path)

    ######################################################
    # 2. è·å–æ•°æ®åº“å®ä¾‹
    # store = GLOBAL_VECTOR_STORE
    if store is None:
        print("âŒ Vector Store is not initialized. Skipping search.")
        logger.error("âŒ Vector Store is not initialized. Skipping search.")
        return {"retrieved_knowledge": [{"error": "Knowledge base not ready"}]}

    # 3. å‡†å¤‡æ£€ç´¢å‚æ•°
    core_function = user_intent.get("core_function", "")
    base_model = user_intent.get("base_model", "")
    details = " ".join(user_intent.get("details", []))
    expected_nodes_str = " ".join(planning.get("expected_nodes", []))

    # æ„å»ºæŸ¥è¯¢è¯
    search_query = f"{base_model} {core_function} {details} {expected_nodes_str}".strip()
    print(f"ğŸ” Search Query: '{search_query}'")

    knowledge_results: List[Dict] = []
    target_node_ids = set()  # ä½¿ç”¨é›†åˆè‡ªåŠ¨å»é‡

    # è·å–è§„åˆ’ä¸­å»ºè®®çš„èŠ‚ç‚¹
    planned_nodes = planning.get("expected_nodes", [])
    if planned_nodes:
        target_node_ids.update(planned_nodes)
        print(f"ğŸ¤– [Planner] Suggested nodes: {planned_nodes}")

    # =================================================
    # 4. åˆ†æ”¯é€»è¾‘ï¼šå·²æœ‰å·¥ä½œæµ / ä»é›¶å¼€å§‹
    # =================================================
    current_workflow = state.current_workflow

    # æ‰“å°ä¸€ä¸‹å·¥ä½œæµçŠ¶æ€åˆ¤æ–­ç»“æœ
    is_empty = is_workflow_empty(current_workflow)
    has_valid_workflow = current_workflow and not is_empty
    print(f"ğŸ“‚ Workflow Status Check: Exists={bool(current_workflow)}, IsEmpty={is_empty}, Valid={has_valid_workflow}")

    has_context = False  # æ ‡è®°æ˜¯å¦æ‰¾åˆ°äº†ä¸Šä¸‹æ–‡

    if has_valid_workflow:
        # --- åˆ†æ”¯ A: åŸºäºç°æœ‰å·¥ä½œæµ ---
        print("ğŸ‘‰ Branch A: Analyzing Existing Workflow")
        extracted_nodes = extract_node_types_from_workflow(current_workflow)

        if extracted_nodes:
            target_node_ids.update(extracted_nodes)
            has_context = True
            print(f"âœ… Extracted {len(extracted_nodes)} unique node types from current workflow.")
            # print(f"   Nodes: {extracted_nodes[:5]}...") # å¯é€‰ï¼šæ‰“å°éƒ¨åˆ†èŠ‚ç‚¹å
    else:
        # --- åˆ†æ”¯ B: æ£€ç´¢å·¥ä½œæµæ¨¡ç‰ˆ ---
        print("ğŸ‘‰ Branch B: Searching for Workflow Templates")

        if search_query:
            wf_results = store.query_workflows(search_query, n=1, threshold=0.6)

            if wf_results and wf_results['ids'] and wf_results['ids'][0]:
                # ğŸ¯ å‘½ä¸­æ¨¡ç‰ˆ
                doc = wf_results['documents'][0][0]
                meta = wf_results['metadatas'][0][0]
                filename = meta.get('filename', 'Unknown')

                print(f"ğŸ‰ Template HIT: {filename}")
                has_context = True

                # æ·»åŠ æ¨¡ç‰ˆæ–‡æ¡£
                knowledge_results.append({
                    "type": "workflow_template",
                    "source": filename,
                    "content": doc,
                    "metadata": meta
                })

                # ä» metadata æå–èŠ‚ç‚¹åˆ—è¡¨
                node_str = meta.get('nodes', '')
                if node_str:
                    template_nodes = [n.strip() for n in node_str.split(',') if n.strip()]
                    target_node_ids.update(template_nodes)
                    print(f"ğŸ”— Extracted {len(template_nodes)} nodes from template metadata.")
            else:
                print("âŒ No matching workflow template found.")

    # =================================================
    # 5. ç²¾ç¡®èŠ‚ç‚¹æ£€ç´¢ (Exact Node Lookup)
    # =================================================
    if target_node_ids:
        print(f"ğŸ•µï¸ [Exact Search] Looking up definitions for {len(target_node_ids)} target nodes...")

        # è¿‡æ»¤æ‰æ— éœ€è§£é‡Šçš„é€šç”¨èŠ‚ç‚¹ä»¥èŠ‚çœ Token
        ignored = ["Note", "Reroute", "Primitive", "PreviewImage", "SaveImage", "Pad"]
        filtered_ids = [nid for nid in target_node_ids if nid not in ignored]

        if len(target_node_ids) != len(filtered_ids):
            print(f"   (Ignored {len(target_node_ids) - len(filtered_ids)} common utility nodes)")

        if hasattr(store, 'get_nodes_by_ids'):
            node_results = store.get_nodes_by_ids(filtered_ids)
            if node_results and node_results['documents']:
                count = len(node_results['documents'])
                print(f"âœ… Successfully retrieved definitions for {count} nodes.")

                # æ£€æŸ¥æœ‰å“ªäº›æ²¡æ‰¾åˆ° (ä¾¿äºè°ƒè¯•)
                found_ids = set()  # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¿”å›ç»“æ„è§£æIDï¼Œç®€å•èµ·è§åªæ‰“å°æ•°é‡
                # å¦‚æœä½ æƒ³çœ‹å…·ä½“çš„: found_ids = set(m['node_id'] for m in node_results['metadatas'])
                # missing = set(filtered_ids) - found_ids
                # if missing: print(f"âš ï¸ Missing definitions for: {missing}")

                for i, doc in enumerate(node_results['documents']):
                    knowledge_results.append({
                        "type": "node_spec",
                        "source": "catalog_exact_match",
                        "content": doc,
                        "metadata": node_results['metadatas'][i]
                    })
            else:
                logger.warning("âš ï¸ Target nodes not found in Catalog (Maybe missing custom nodes?).")
                print("âš ï¸ No definitions found for target IDs.")

    # =================================================
    # 6. å…œåº•é€»è¾‘ï¼šæ¨¡ç³Šæœç´¢ (Fuzzy Search)
    # =================================================
    if not has_context or len(knowledge_results) < 2:
        print("ğŸ”¸ Context insufficient (Low confidence). Performing fuzzy node search...")

        if search_query:
            fuzzy_results = store.query_nodes(search_query, n_results=3, threshold=0.5)
            if fuzzy_results and fuzzy_results['documents']:
                count = len(fuzzy_results['documents'][0])
                print(f"ğŸŒ«ï¸ Fuzzy search found {count} related nodes.")

                for i, doc in enumerate(fuzzy_results['documents'][0]):
                    knowledge_results.append({
                        "type": "node_spec",
                        "source": "catalog_fuzzy_search",
                        "content": doc,
                        "metadata": fuzzy_results['metadatas'][0][i]
                    })
            else:
                print("âŒ Fuzzy search returned no results.")

    # å»é‡
    unique_knowledge = []
    seen_content = set()
    for item in knowledge_results:
        content_hash = hash(item['content'])
        if content_hash not in seen_content:
            unique_knowledge.append(item)
            seen_content.add(content_hash)

    print(f"ğŸ [Search] Retrieval Complete. Returning {len(unique_knowledge)} unique items.")
    print("=" * 50 + "\n")

    return {"retrieved_knowledge": unique_knowledge}
