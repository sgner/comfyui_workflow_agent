import os
import uuid
import logging
import chromadb
from chromadb.config import Settings
import re
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # å›½å†…æœ€å¿«çš„ HuggingFace é•œåƒ
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "120"
os.environ["HF_HUB_ETAG_TIMEOUT"] = "60"
logger = logging.getLogger("WorkflowAgent")


class ChromaVectorStore:
    def __init__(self, persist_directory):
        """
        åˆå§‹åŒ– ChromaDB å‘é‡åº“
        :param persist_directory: æ•°æ®åº“æŒä¹…åŒ–è·¯å¾„
        """
        if not os.path.exists(persist_directory):
            os.makedirs(persist_directory)

        self.persist_directory = persist_directory
        self.node_collection_name = "comfyui_nodes_catalog"

        # 1. æ£€æŸ¥å¿…è¦ä¾èµ–
        try:
            from chromadb.utils import embedding_functions
            import sentence_transformers  # ç¡®ä¿å·²å®‰è£…
        except ImportError:
            raise ImportError("Missing dependencies. Please run: pip install chromadb sentence-transformers")

        # 2. åˆå§‹åŒ–å®¢æˆ·ç«¯ (å¸¦å†²çªè§£å†³æœºåˆ¶)
        # ComfyUI é‡è½½æ—¶å¯èƒ½ä¼šå¯¼è‡´æ—§çš„ Client å¯¹è±¡æœªé‡Šæ”¾ï¼Œå†æ¬¡åˆå§‹åŒ–ä¼šæŠ¥é”™
        try:
            self.client = chromadb.PersistentClient(path=persist_directory)
        except ValueError as e:
            if "already exists" in str(e):
                logger.warning("âš ï¸ Chroma Client conflict detected. Attaching to existing session...")
                try:
                    # å°è¯•è¿æ¥åˆ°å·²å­˜åœ¨çš„ç³»ç»Ÿä¼šè¯
                    self.client = chromadb.Client(Settings(
                        is_persistent=True,
                        persist_directory=persist_directory
                    ))
                except Exception as sub_e:
                    logger.error(f"âŒ Failed to recover Chroma Client: {sub_e}")
                    raise e
            else:
                raise e

        # 3. åˆå§‹åŒ– Embedding æ¨¡å‹ (CPUæ¨¡å¼ï¼Œä¸å æ˜¾å­˜)
        logger.info("ğŸ“¦ [Catalog] Loading embedding model (all-MiniLM-L6-v2)...")
        base_path = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        local_model_path = os.path.join(base_path, "models", "all-MiniLM-L6-v2")
        model_name_or_path = local_model_path if os.path.exists(local_model_path) else "all-MiniLM-L6-v2"

        if os.path.exists(local_model_path):
            logger.info(f"   -> Using local model at: {local_model_path}")
        else:
            logger.info(f"   -> Local model not found, trying to download 'all-MiniLM-L6-v2'...")

        self.embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
            model_name=model_name_or_path,
            device="cpu"
        )

        # 4. è·å–æˆ–åˆ›å»ºé›†åˆ
        self.node_collection = self.client.get_or_create_collection(
            name=self.node_collection_name,
            embedding_function=self.embedding_fn
        )

        self.workflow_collection = self.client.get_or_create_collection(
            name="comfyui_workflows_gallery",
            embedding_function=self.embedding_fn
        )

    def process_incremental_update(self, docs_to_upsert, ids_to_delete):
        """
        æ‰§è¡Œå¢é‡æ›´æ–°
        :param docs_to_upsert: éœ€è¦æ–°å¢æˆ–æ›´æ–°çš„æ–‡æ¡£åˆ—è¡¨ (List[Dict])
        :param ids_to_delete: éœ€è¦åˆ é™¤çš„èŠ‚ç‚¹ ID åˆ—è¡¨ (List[str])
        """
        try:
            # === 1. åˆ é™¤æ“ä½œ ===
            if ids_to_delete:
                logger.info(f"ğŸ—‘ï¸ [Catalog] Deleting {len(ids_to_delete)} obsolete nodes from DB...")
                # ç¡®ä¿ ID æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨
                safe_ids = [str(i) for i in ids_to_delete]
                self.node_collection.delete(ids=safe_ids)

            # === 2. æ›´æ–°/æ’å…¥æ“ä½œ (Upsert) ===
            if docs_to_upsert:
                count = len(docs_to_upsert)
                logger.info(f"ğŸ”„ [Catalog] Upserting {count} nodes to DB...")

                # æå–æ•°æ®
                ids = []
                documents = []
                metadatas = []

                for doc in docs_to_upsert:
                    # ç¡®ä¿ ID æ˜¯å­—ç¬¦ä¸²
                    node_id = str(doc['metadata'].get('node_id', uuid.uuid4()))
                    ids.append(node_id)

                    documents.append(doc['content'])

                    # æ¸…æ´— metadataï¼Œç¡®ä¿æ²¡æœ‰å¤æ‚å¯¹è±¡ (Chroma ä¸æ”¯æŒ List/Dict ä½œä¸º value)
                    safe_meta = {}
                    for k, v in doc['metadata'].items():
                        if isinstance(v, (str, int, float, bool)):
                            safe_meta[k] = v
                        else:
                            safe_meta[k] = str(v)  # å¼ºè½¬ä¸ºå­—ç¬¦ä¸²
                    metadatas.append(safe_meta)

                # æ‰¹å¤„ç†å†™å…¥ (é˜²æ­¢ SQLite é”æ­»æˆ–å†…å­˜æº¢å‡º)
                batch_size = 50
                for i in range(0, count, batch_size):
                    end = min(i + batch_size, count)
                    self.node_collection.upsert(
                        ids=ids[i:end],
                        documents=documents[i:end],
                        metadatas=metadatas[i:end]
                    )
                    # logger.info(f"   Batch {i}-{end} written.")

            logger.info("âœ… [Catalog] DB Update successful.")
            return True

        except Exception as e:
            logger.error(f"âŒ [Catalog] Update failed: {e}")
            import traceback
            traceback.print_exc()
            return False

    def query_nodes(self, query_text, n_results=5, where_filter=None, threshold=0.55):
        """
        æŸ¥è¯¢èŠ‚ç‚¹ (å¸¦é˜ˆå€¼è¿‡æ»¤)
        :param threshold: è·ç¦»é˜ˆå€¼ï¼Œé»˜è®¤ 0.55ã€‚è¶Šå°è¶Šä¸¥æ ¼ã€‚
                          0.3 = éå¸¸ç›¸ä¼¼
                          0.5 = ç›¸å…³
                          >0.7 = åŸºæœ¬ä¸ç›¸å…³
        """
        try:
            results = self.node_collection.query(
                query_texts=[query_text],
                n_results=n_results,
                where=where_filter
            )

            return self._filter_results(results, threshold)

        except Exception as e:
            logger.error(f"âŒ [Catalog] Node query failed: {e}")
            return None

    def reset_and_rebuild(self, documents):
        """
        [ç¾éš¾æ¢å¤ç”¨] å¼ºåˆ¶æ¸…ç©ºå¹¶å…¨é‡é‡å»º
        """
        try:
            logger.warning("ğŸ”¥ [Catalog] Performing full DB reset...")
            try:
                self.client.delete_collection(self.node_collection_name)
            except:
                pass

            self.node_collection = self.client.create_collection(
                name=self.node_collection_name,
                embedding_function=self.embedding_fn
            )

            # å¤ç”¨å¢é‡é€»è¾‘è¿›è¡Œå…¨é‡å†™å…¥
            self.process_incremental_update(documents, [])

        except Exception as e:
            logger.error(f"âŒ [Catalog] Reset failed: {e}")

    def process_workflow_update(self, workflows_to_upsert, filenames_to_delete):
        """
            å·¥ä½œæµå¢é‡æ›´æ–°
            :param workflows_to_upsert: éœ€è¦æ›´æ–°/æ–°å¢çš„å·¥ä½œæµæ•°æ®åˆ—è¡¨ (æ¥è‡ª Scanner)
            :param filenames_to_delete: éœ€è¦åˆ é™¤çš„æ–‡ä»¶ååˆ—è¡¨
            """
        try:
            # 1. åˆ é™¤
            if filenames_to_delete:
                logger.info(f"ğŸ—‘ï¸ [Workflow] Removing {len(filenames_to_delete)} templates...")
                self.workflow_collection.delete(ids=filenames_to_delete)

            # 2. æ›´æ–°/æ–°å¢
            if workflows_to_upsert:
                logger.info(f"ğŸ”„ [Workflow] Upserting {len(workflows_to_upsert)} templates...")

                ids = []
                docs = []
                metadatas = []

                for wf in workflows_to_upsert:
                    filename = wf['filename']
                    ids.append(filename)

                    # æ„å»ºå¯Œå«è¯­ä¹‰çš„ Embedding æ–‡æœ¬
                    content = f"""
    # Workflow Template: {filename}
    ## Models Used
    {', '.join(wf['models_used'])}

    ## Key Nodes
    {', '.join(wf['node_types'])}

    ## Description
    This is a reference workflow containing {len(wf['node_types'])} nodes.
    It is optimized for models: {', '.join(wf['models_used'])}.
    """
                    docs.append(content)

                    # æ‰å¹³åŒ– Metadata (Chroma ä¸æ”¯æŒåˆ—è¡¨)
                    metadatas.append({
                        "filename": filename,
                        "models": ", ".join(wf['models_used'])[:1000],  # é˜²æ­¢è¿‡é•¿
                        "nodes": ", ".join(wf['node_types'])[:1000]
                    })

                # æ‰¹é‡å†™å…¥
                if ids:
                    self.workflow_collection.upsert(
                        ids=ids,
                        documents=docs,
                        metadatas=metadatas
                    )

            logger.info("âœ… [Workflow] Gallery updated successfully.")
            return True

        except Exception as e:
            logger.error(f"âŒ [Workflow] Update failed: {e}")
            return False

    def query_workflows(self, query, n=5, threshold=0.6):
        """
        æ··åˆæ£€ç´¢ï¼šå‘é‡æ£€ç´¢ + å…³é”®è¯æ£€ç´¢
        """
        combined_results = {
            'ids': [[]],
            'distances': [[]],
            'metadatas': [[]],
            'documents': [[]]
        }

        seen_ids = set()

        # -------------------------------------------------
        # 1. å…³é”®è¯æ£€ç´¢ (Keyword Search) - ä¼˜å…ˆ
        # -------------------------------------------------
        kw_results = self._keyword_search_workflows(query)
        if kw_results:
            for i, doc_id in enumerate(kw_results['ids']):
                if doc_id not in seen_ids:
                    combined_results['ids'][0].append(doc_id)
                    combined_results['distances'][0].append(0.0)  # å¼ºåˆ¶ç½®é¡¶
                    combined_results['metadatas'][0].append(kw_results['metadatas'][i])
                    combined_results['documents'][0].append(kw_results['documents'][i])
                    seen_ids.add(doc_id)

            logger.info(f"ğŸ” [Hybrid] Keyword match found {len(kw_results['ids'])} items.")

        # -------------------------------------------------
        # 2. å‘é‡æ£€ç´¢ (Vector Search) - è¡¥å……
        # -------------------------------------------------
        try:
            vec_results = self.workflow_collection.query(
                query_texts=[query],
                n_results=n
            )

            # è¿‡æ»¤å¹¶åˆå¹¶
            filtered_vec = self._filter_results(vec_results, threshold)
            if filtered_vec:
                for i, doc_id in enumerate(filtered_vec['ids'][0]):
                    if doc_id not in seen_ids:
                        combined_results['ids'][0].append(doc_id)
                        combined_results['distances'][0].append(filtered_vec['distances'][0][i])
                        combined_results['metadatas'][0].append(filtered_vec['metadatas'][0][i])
                        combined_results['documents'][0].append(filtered_vec['documents'][0][i])
                        seen_ids.add(doc_id)

        except Exception as e:
            logger.error(f"âŒ [Workflow] Vector query failed: {e}")

        # -------------------------------------------------
        # 3. æˆªæ–­ç»“æœ
        # -------------------------------------------------
        # åªè¿”å›å‰ N ä¸ª
        if len(combined_results['ids'][0]) > n:
            combined_results['ids'][0] = combined_results['ids'][0][:n]
            combined_results['distances'][0] = combined_results['distances'][0][:n]
            combined_results['metadatas'][0] = combined_results['metadatas'][0][:n]
            combined_results['documents'][0] = combined_results['documents'][0][:n]

        # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œè¿”å› None
        if not combined_results['ids'][0]:
            return None

        return combined_results

    def _filter_results(self, results, threshold):
        """
        å†…éƒ¨æ–¹æ³•ï¼šæ ¹æ®è·ç¦»é˜ˆå€¼è¿‡æ»¤ Chroma è¿”å›çš„åŸå§‹ç»“æœ
        """
        if not results or not results['ids'] or not results['distances']:
            return None

        # Chroma è¿”å›çš„æ˜¯ Batch åˆ—è¡¨ [[id1, id2...]]ï¼Œæˆ‘ä»¬åªå¤„ç† Batch 0
        original_ids = results['ids'][0]
        original_distances = results['distances'][0]
        original_metadatas = results['metadatas'][0]
        original_documents = results['documents'][0]

        # ç­›é€‰åˆæ ¼çš„ç´¢å¼•
        valid_indices = [
            i for i, dist in enumerate(original_distances)
            if dist < threshold
        ]

        if not valid_indices:
            return None  # å¦‚æœæ²¡æœ‰ä¸€ä¸ªåˆæ ¼çš„ï¼Œç›´æ¥è¿”å› None

        # é‡æ„è¿”å›ç»“æ„
        filtered_ids = [original_ids[i] for i in valid_indices]
        filtered_distances = [original_distances[i] for i in valid_indices]
        filtered_metadatas = [original_metadatas[i] for i in valid_indices]
        filtered_documents = [original_documents[i] for i in valid_indices]

        # ä¿æŒ Chroma çš„è¿”å›æ ¼å¼ (Batch List)
        return {
            'ids': [filtered_ids],
            'distances': [filtered_distances],
            'metadatas': [filtered_metadatas],
            'documents': [filtered_documents]
        }

    def get_nodes_by_ids(self, node_ids):
        """
        æ ¹æ®èŠ‚ç‚¹ ID åˆ—è¡¨è·å–èŠ‚ç‚¹çš„è¯¦ç»†å®šä¹‰ (éå‘é‡æœç´¢ï¼Œè€Œæ˜¯ç²¾ç¡®ä¸»é”®æŸ¥æ‰¾)
        :param node_ids: èŠ‚ç‚¹ ID åˆ—è¡¨ï¼Œå¦‚ ['KSampler', 'SaveImage']
        :return: Chroma GetResult å­—å…¸
        """
        try:
            # è¿‡æ»¤ç©ºå€¼å’Œé‡å¤å€¼
            unique_ids = list(set([nid.strip() for nid in node_ids if nid and nid.strip()]))

            if not unique_ids:
                return None

            # ä½¿ç”¨ Chroma çš„ get æ–¹æ³•ç›´æ¥é€šè¿‡ ID è·å–
            results = self.node_collection.get(
                ids=unique_ids,
                include=['documents', 'metadatas']
            )
            return results
        except Exception as e:
            logger.error(f"âŒ [Catalog] Get nodes by ID failed: {e}")
            return None

    def _keyword_search_workflows(self, query):
        """
        [è¾…åŠ©æ–¹æ³•] æ”¹è¿›ç‰ˆï¼šåŸºäºåˆ†è¯çš„å…³é”®è¯æ£€ç´¢
        è§£å†³ "é•¿å¥æœä¸åˆ°çŸ­è¯" çš„é—®é¢˜
        """
        try:
            all_data = self.workflow_collection.get(include=['metadatas', 'documents'])

            if not all_data['ids']:
                return None

            hits = {
                'ids': [],
                'distances': [],
                'metadatas': [],
                'documents': []
            }

            # 1. é¢„å¤„ç†æŸ¥è¯¢è¯ï¼šæå–æ½œåœ¨çš„å…³é”®è¯
            # é€»è¾‘ï¼šæå–æ‰€æœ‰ç”±å­—æ¯ã€æ•°å­—ã€æ¨ªæ ã€ç‚¹ã€ä¸‹åˆ’çº¿ç»„æˆçš„åºåˆ— (é€šå¸¸æ˜¯æ¨¡å‹åã€æ–‡ä»¶åã€ç‰ˆæœ¬å·)
            # ä¾‹å¦‚: "ä½¿ç”¨ z-image-turbo q8" -> ['z-image-turbo', 'q8']
            query_lower = query.lower()

            # æ­£åˆ™æå–è‹±æ–‡/æ•°å­—å…³é”®è¯ (è¿‡æ»¤æ‰çº¯ä¸­æ–‡æè¿°ï¼Œå› ä¸ºmetadataé‡Œé€šå¸¸åªæœ‰è‹±æ–‡æ¨¡å‹å)
            potential_keywords = set(re.findall(r'[a-zA-Z0-9\-\._]+', query_lower))

            # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¯ (å¦‚ "v1", "a" ç­‰å®¹æ˜“è¯¯åˆ¤çš„ï¼Œè§†æƒ…å†µè°ƒæ•´)
            keywords = [k for k in potential_keywords if len(k) >= 2]

            # å¦‚æœæ²¡æå–åˆ°è‹±æ–‡å…³é”®è¯ï¼Œå›é€€åˆ°æŒ‰ç©ºæ ¼åˆ†å‰² (é’ˆå¯¹çº¯ä¸­æ–‡ç¯å¢ƒçš„è¡¥å……)
            if not keywords:
                keywords = query_lower.split()

            if not keywords:
                return None

            # 2. éå†åŒ¹é…
            for idx, meta in enumerate(all_data['metadatas']):
                # æ‹¼æ¥ç›®æ ‡çš„æœç´¢åŸŸ
                filename = meta.get('filename', '').lower()
                models = meta.get('models', '').lower()
                # è¿˜å¯ä»¥åŠ å…¥ nodes åˆ—è¡¨è¾…åŠ©åŒ¹é…
                nodes = meta.get('nodes', '').lower()

                target_text = f"{filename} {models} {nodes}"

                # 3. æ ¸å¿ƒåŒ¹é…é€»è¾‘ï¼šåªè¦å‘½ä¸­ã€ä»»ä½•ä¸€ä¸ªã€‘å…³é”®é•¿è¯ï¼Œå°±ç®—åŒ¹é…
                # ä¹Ÿå¯ä»¥æ”¹ä¸ºã€å‘½ä¸­æ‰€æœ‰ã€‘æˆ–è€…ã€æ‰“åˆ†åˆ¶ã€‘
                is_hit = False
                for kw in keywords:
                    # æ’é™¤ä¸€äº›æ— æ„ä¹‰çš„å¸¸ç”¨è¯ (å¯æ ¹æ®éœ€è¦æ‰©å±•)
                    if kw in ["json", "workflow", "comfyui", "model", "use", "create"]:
                        continue

                    if kw in target_text:
                        is_hit = True
                        break  # å‘½ä¸­ä¸€ä¸ªæ ¸å¿ƒè¯å³å¯ï¼Œæ¯”å¦‚å‘½ä¸­äº† "z-image-turbo"

                if is_hit:
                    hits['ids'].append(all_data['ids'][idx])
                    hits['metadatas'].append(meta)
                    hits['documents'].append(all_data['documents'][idx])
                    hits['distances'].append(0.0)  # å…³é”®è¯å‘½ä¸­ï¼Œç½®é¡¶

            return hits if hits['ids'] else None

        except Exception as e:
            logger.error(f"âŒ [Workflow] Keyword search failed: {e}")
            return None
